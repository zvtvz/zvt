# ZVT Crypto Database Migration Strategy

**Version**: v1.1  
**Date**: 2025-08-18  
**Status**: Enhanced with Epic 1 Implementation Details

## Overview

This document outlines the database migration strategy for adding crypto market support to ZVT. The strategy ensures zero-downtime deployment, data integrity, and backwards compatibility with existing ZVT functionality.

## Migration Principles

1. **Zero Downtime**: All migrations must be non-breaking and backwards compatible
2. **Data Integrity**: Full ACID compliance with rollback capabilities
3. **Backwards Compatibility**: Existing stock/index functionality unaffected
4. **Incremental Deployment**: Phased rollout with validation at each step
5. **Performance Preservation**: No impact on existing query performance

## Database Schema Changes

### 1. New Database Files (SQLite) / Schemas (MySQL/PostgreSQL)

#### Meta Tables
```sql
-- Crypto asset metadata
CREATE TABLE crypto_asset (
    id VARCHAR(128) PRIMARY KEY,
    entity_id VARCHAR(128),
    entity_type VARCHAR(64) DEFAULT 'crypto',
    exchange VARCHAR(32),
    code VARCHAR(64),
    name VARCHAR(128),
    symbol VARCHAR(32),
    full_name VARCHAR(128),
    max_supply FLOAT,
    circulating_supply FLOAT,
    total_supply FLOAT,
    market_cap FLOAT,
    is_stablecoin BOOLEAN DEFAULT FALSE,
    consensus_mechanism VARCHAR(64),
    list_date DATETIME,
    end_date DATETIME,
    timestamp DATETIME,
    INDEX idx_crypto_asset_code (code),
    INDEX idx_crypto_asset_symbol (symbol),
    INDEX idx_crypto_asset_timestamp (timestamp)
);

-- Crypto spot pairs  
CREATE TABLE crypto_pair (
    id VARCHAR(128) PRIMARY KEY,
    entity_id VARCHAR(128),
    entity_type VARCHAR(64) DEFAULT 'cryptopair',
    exchange VARCHAR(32),
    code VARCHAR(64),
    name VARCHAR(128), 
    base_symbol VARCHAR(32),
    quote_symbol VARCHAR(32),
    base_asset_id VARCHAR(128),
    quote_asset_id VARCHAR(128),
    price_step FLOAT,
    qty_step FLOAT,
    min_notional FLOAT,
    max_order_size FLOAT,
    maker_fee FLOAT,
    taker_fee FLOAT,
    is_active BOOLEAN DEFAULT TRUE,
    margin_enabled BOOLEAN DEFAULT FALSE,
    list_date DATETIME,
    end_date DATETIME,
    timestamp DATETIME,
    INDEX idx_crypto_pair_code (code),
    INDEX idx_crypto_pair_base_quote (base_symbol, quote_symbol),
    INDEX idx_crypto_pair_exchange (exchange),
    INDEX idx_crypto_pair_active (is_active),
    INDEX idx_crypto_pair_timestamp (timestamp),
    FOREIGN KEY (base_asset_id) REFERENCES crypto_asset(id),
    FOREIGN KEY (quote_asset_id) REFERENCES crypto_asset(id)
);

-- Crypto perpetual futures
CREATE TABLE crypto_perp (
    id VARCHAR(128) PRIMARY KEY,
    entity_id VARCHAR(128),
    entity_type VARCHAR(64) DEFAULT 'cryptoperp',
    exchange VARCHAR(32),
    code VARCHAR(64),
    name VARCHAR(128),
    underlying_symbol VARCHAR(32),
    settlement_symbol VARCHAR(32), 
    underlying_asset_id VARCHAR(128),
    settlement_asset_id VARCHAR(128),
    contract_size FLOAT DEFAULT 1.0,
    price_step FLOAT,
    qty_step FLOAT,
    min_notional FLOAT,
    max_order_size FLOAT,
    maker_fee FLOAT,
    taker_fee FLOAT,
    funding_interval_hours INTEGER DEFAULT 8,
    max_leverage FLOAT,
    default_leverage FLOAT DEFAULT 1.0,
    position_modes VARCHAR(64) DEFAULT 'both',
    maintenance_margin_rate FLOAT,
    initial_margin_rate FLOAT,
    is_active BOOLEAN DEFAULT TRUE,
    list_date DATETIME,
    end_date DATETIME,
    timestamp DATETIME,
    INDEX idx_crypto_perp_code (code),
    INDEX idx_crypto_perp_underlying (underlying_symbol),
    INDEX idx_crypto_perp_exchange (exchange),
    INDEX idx_crypto_perp_active (is_active),
    INDEX idx_crypto_perp_timestamp (timestamp),
    FOREIGN KEY (underlying_asset_id) REFERENCES crypto_asset(id),
    FOREIGN KEY (settlement_asset_id) REFERENCES crypto_asset(id)
);
```

#### Kdata Tables (Example - Full list generated by schema generator)
```sql
-- 1-minute crypto pair kdata
CREATE TABLE cryptopair_1m_kdata (
    id VARCHAR(128) PRIMARY KEY,
    entity_id VARCHAR(128),
    provider VARCHAR(32),
    code VARCHAR(32),
    name VARCHAR(32),
    level VARCHAR(32) DEFAULT '1m',
    open FLOAT,
    high FLOAT, 
    low FLOAT,
    close FLOAT,
    volume FLOAT,
    turnover FLOAT,
    change_pct FLOAT,
    turnover_rate FLOAT,
    volume_base FLOAT,
    volume_quote FLOAT,
    trade_count INTEGER,
    vwap FLOAT,
    is_high_volatility BOOLEAN DEFAULT FALSE,
    is_high_volume BOOLEAN DEFAULT FALSE,
    timestamp DATETIME,
    INDEX idx_cryptopair_1m_entity_timestamp (entity_id, timestamp),
    INDEX idx_cryptopair_1m_code_timestamp (code, timestamp),
    INDEX idx_cryptopair_1m_provider (provider),
    INDEX idx_cryptopair_1m_timestamp (timestamp),
    FOREIGN KEY (entity_id) REFERENCES crypto_pair(id)
);

-- Similar tables for other intervals: 5m, 15m, 30m, 1h, 4h, 1d
-- Similar tables for crypto_perp: cryptoperp_1m_kdata, cryptoperp_5m_kdata, etc.
```

#### Tick-level Tables
```sql
-- Individual crypto trades
CREATE TABLE crypto_trade (
    id VARCHAR(128) PRIMARY KEY,
    entity_id VARCHAR(128),
    provider VARCHAR(32),
    code VARCHAR(32),
    name VARCHAR(32),
    trade_id VARCHAR(128),
    price FLOAT,
    volume FLOAT,
    quote_volume FLOAT,
    side VARCHAR(16),
    is_buyer_maker BOOLEAN,
    bid_price FLOAT,
    ask_price FLOAT,
    spread FLOAT,
    trade_flags VARCHAR(64),
    timestamp_ms BIGINT,
    timestamp DATETIME,
    INDEX idx_crypto_trade_entity_timestamp (entity_id, timestamp),
    INDEX idx_crypto_trade_trade_id (trade_id),
    INDEX idx_crypto_trade_timestamp_ms (timestamp_ms),
    INDEX idx_crypto_trade_side (side)
);

-- Order book snapshots
CREATE TABLE crypto_orderbook (
    id VARCHAR(128) PRIMARY KEY,
    entity_id VARCHAR(128),
    provider VARCHAR(32),
    code VARCHAR(32),
    name VARCHAR(32),
    update_id BIGINT,
    prev_update_id BIGINT,
    first_update_id BIGINT,
    last_update_id BIGINT,
    bids JSON,
    asks JSON,
    bid_levels INTEGER,
    ask_levels INTEGER,
    best_bid FLOAT,
    best_ask FLOAT,
    spread FLOAT,
    spread_pct FLOAT,
    mid_price FLOAT,
    bid_volume FLOAT,
    ask_volume FLOAT,
    bid_vwap FLOAT,
    ask_vwap FLOAT,
    checksum VARCHAR(64),
    is_snapshot BOOLEAN DEFAULT FALSE,
    timestamp_ms BIGINT,
    timestamp DATETIME,
    INDEX idx_crypto_orderbook_entity_timestamp (entity_id, timestamp),
    INDEX idx_crypto_orderbook_update_id (update_id),
    INDEX idx_crypto_orderbook_timestamp_ms (timestamp_ms)
);

-- Perpetual funding rates
CREATE TABLE crypto_funding (
    id VARCHAR(128) PRIMARY KEY,
    entity_id VARCHAR(128),
    provider VARCHAR(32),
    code VARCHAR(32),
    name VARCHAR(32),
    funding_rate FLOAT,
    predicted_rate FLOAT,
    funding_timestamp DATETIME,
    next_funding_timestamp DATETIME,
    funding_interval_hours INTEGER DEFAULT 8,
    mark_price FLOAT,
    index_price FLOAT,
    premium_rate FLOAT,
    interest_rate FLOAT,
    funding_cost_long FLOAT,
    funding_cost_short FLOAT,
    avg_funding_24h FLOAT,
    max_funding_24h FLOAT,
    min_funding_24h FLOAT,
    is_bullish_funding BOOLEAN,
    is_extreme_funding BOOLEAN,
    calculation_timestamp DATETIME,
    timestamp_ms BIGINT,
    timestamp DATETIME,
    INDEX idx_crypto_funding_entity_timestamp (entity_id, timestamp),
    INDEX idx_crypto_funding_funding_timestamp (funding_timestamp),
    INDEX idx_crypto_funding_next_funding (next_funding_timestamp)
);
```

### 2. Migration Sequence

#### Phase 1: Schema Creation (Week 1)
1. **Create new database files/schemas**
   - `crypto_meta.db` (SQLite) or `crypto_meta` schema
   - `crypto_tick.db` (SQLite) or `crypto_tick` schema
   - Individual kdata databases per interval/type

2. **Validate schema creation**
   - Run DDL scripts in test environment
   - Verify index creation and performance
   - Test foreign key constraints

3. **Update ZVT configuration**
   - Add crypto providers to config
   - Update database connection settings
   - Configure schema registration

#### Phase 2: Code Integration (Week 2)
1. **Deploy crypto domain code**
   - Entity classes (CryptoAsset, CryptoPair, CryptoPerp)
   - Schema classes (auto-generated kdata schemas)
   - Calendar integration

2. **Test basic functionality**
   - Entity creation and querying
   - Schema registration verification
   - API endpoint exposure

3. **Data validation**
   - Test data insertion
   - Query performance testing
   - Cross-table relationship validation

#### Phase 3: Provider Integration (Weeks 3-4)
1. **Deploy first provider (Binance testnet)**
   - Metadata population
   - Historical data backfill (limited)
   - Real-time data streaming (test mode)

2. **Data quality validation**
   - Cross-validate with external sources
   - Gap detection and monitoring
   - Performance benchmarking

## Migration Scripts

### Schema Creation Script
```python
# migrations/001_create_crypto_schemas.py
from zvt.contract.api import get_db_engine
from zvt.domain.crypto import CryptoAsset, CryptoPair, CryptoPerp
from sqlalchemy import create_engine

def upgrade():
    """Create crypto database schemas"""
    
    # Create meta schema
    engine = get_db_engine(provider='zvt', data_schema=CryptoAsset)
    CryptoAsset.metadata.create_all(engine)
    CryptoPair.metadata.create_all(engine) 
    CryptoPerp.metadata.create_all(engine)
    
    # Create tick schema
    from zvt.domain.crypto import CryptoTrade, CryptoOrderbook, CryptoFunding
    tick_engine = get_db_engine(provider='zvt', data_schema=CryptoTrade)
    CryptoTrade.metadata.create_all(tick_engine)
    CryptoOrderbook.metadata.create_all(tick_engine)
    CryptoFunding.metadata.create_all(tick_engine)
    
    print("‚úÖ Crypto schemas created successfully")

def downgrade():
    """Remove crypto database schemas"""
    
    # Drop all crypto tables
    engine = get_db_engine(provider='zvt', data_schema=CryptoAsset)
    CryptoAsset.metadata.drop_all(engine)
    CryptoPair.metadata.drop_all(engine)
    CryptoPerp.metadata.drop_all(engine)
    
    tick_engine = get_db_engine(provider='zvt', data_schema=CryptoTrade)
    CryptoTrade.metadata.drop_all(tick_engine)
    CryptoOrderbook.metadata.drop_all(tick_engine)
    CryptoFunding.metadata.drop_all(tick_engine)
    
    print("‚úÖ Crypto schemas removed successfully")
```

### Data Migration Utilities
```python
# migrations/utils/crypto_migration_utils.py
import pandas as pd
from typing import List, Dict
from zvt.contract.api import df_to_db

class CryptoMigrationValidator:
    """Utilities for validating crypto data migrations"""
    
    @staticmethod
    def validate_entity_data(provider: str, entity_type: str) -> Dict:
        """Validate entity data integrity"""
        from zvt.domain.crypto import CryptoAsset, CryptoPair, CryptoPerp
        
        entity_class = {
            'crypto': CryptoAsset,
            'cryptopair': CryptoPair,
            'cryptoperp': CryptoPerp
        }[entity_type]
        
        # Check data counts, duplicate IDs, foreign key integrity
        df = entity_class.query_data(provider=provider)
        
        return {
            'total_records': len(df),
            'duplicate_ids': df.duplicated(subset=['id']).sum(),
            'null_ids': df['id'].isnull().sum(),
            'null_entity_ids': df['entity_id'].isnull().sum()
        }
    
    @staticmethod
    def validate_kdata_integrity(provider: str, schema_class) -> Dict:
        """Validate kdata integrity and continuity"""
        df = schema_class.query_data(provider=provider, limit=10000)
        
        if len(df) == 0:
            return {'status': 'no_data'}
            
        # Check for gaps, duplicates, data quality
        df = df.sort_values(['entity_id', 'timestamp'])
        
        return {
            'total_records': len(df),
            'entity_count': df['entity_id'].nunique(),
            'date_range': (df['timestamp'].min(), df['timestamp'].max()),
            'duplicate_timestamps': df.duplicated(subset=['entity_id', 'timestamp']).sum(),
            'null_ohlc': df[['open', 'high', 'low', 'close']].isnull().sum().sum()
        }
```

## Rollback Strategy

### Immediate Rollback (if issues detected in Phase 1-2)
1. **Drop crypto schemas**
   - Execute downgrade migration script
   - Remove crypto database files
   - Revert configuration changes

2. **Code rollback**
   - Remove crypto domain modules
   - Revert API endpoints
   - Remove provider registrations

### Data Rollback (if issues detected in Phase 3)
1. **Preserve schema, clear data**
   - Truncate crypto tables
   - Reset auto-increment counters
   - Clear provider registrations

2. **Selective rollback**
   - Remove specific provider data only
   - Keep schema and code changes
   - Allow retry with different provider

## Performance Considerations

### Indexing Strategy
- **Primary indexes**: entity_id + timestamp for all time-series tables
- **Secondary indexes**: code, provider, exchange for filtering
- **Composite indexes**: Multi-column indexes for common query patterns
- **Partial indexes**: Active-only entities, specific date ranges

### Query Optimization
- **Partition large tables** by date for time-series data
- **Optimize JOIN queries** between entities and kdata
- **Index foreign keys** for referential integrity
- **Monitor query performance** with EXPLAIN ANALYZE

### Storage Optimization
- **Data compression** for historical kdata tables
- **Archive old data** beyond retention period
- **Optimize JSON columns** for orderbook data
- **Regular VACUUM/OPTIMIZE** for SQLite databases

## Monitoring & Validation

### Migration Health Checks
```python
# Health check script
def crypto_migration_health_check():
    checks = [
        validate_schema_creation(),
        validate_entity_relationships(), 
        validate_data_integrity(),
        validate_api_endpoints(),
        validate_provider_registration(),
        validate_query_performance()
    ]
    
    return {
        'overall_status': all(c['status'] == 'pass' for c in checks),
        'individual_checks': checks
    }
```

### Performance Monitoring
- **Query response times** before/after migration
- **Database size growth** tracking
- **Index usage statistics**
- **Connection pool utilization**

## Risk Mitigation

### Pre-Migration Testing
1. **Full schema testing** in isolated environment
2. **Performance benchmarking** with synthetic data
3. **Rollback procedure verification**
4. **API endpoint testing**

### Production Deployment
1. **Blue-green deployment** for zero downtime
2. **Canary release** with limited user base
3. **Real-time monitoring** of key metrics
4. **Automated rollback triggers** for critical failures

### Post-Migration Validation
1. **Data integrity checks** every 6 hours
2. **Performance regression testing** daily
3. **Error rate monitoring** with alerting
4. **User acceptance testing** with beta users

## Success Criteria

### Functional Requirements
- ‚úÖ All crypto schemas created without errors
- ‚úÖ Entity relationships properly established
- ‚úÖ API endpoints accessible and responsive
- ‚úÖ Provider registration successful

### Performance Requirements  
- ‚úÖ Query response time < 100ms for standard queries
- ‚úÖ No performance regression for existing functionality
- ‚úÖ Database size growth within expected bounds
- ‚úÖ Index effectiveness > 95%

### Data Quality Requirements
- ‚úÖ Zero data integrity violations
- ‚úÖ < 0.1% duplicate records
- ‚úÖ Foreign key constraints enforced
- ‚úÖ Data validation rules passing

This migration strategy ensures a safe, reversible deployment of crypto market support while maintaining ZVT's reliability and performance standards.

## Epic 1 Implementation Enhancements

### Validated Schema Generation Integration

#### Automated Schema Creation with Epic 1 Patterns
```python
# Enhanced migration script using Epic 1 validated patterns
# migrations/002_enhanced_crypto_schemas.py

from zvt.fill_crypto_project import gen_crypto_kdata_schemas
from zvt.domain.crypto import CryptoAsset, CryptoPair, CryptoPerp

def upgrade_with_epic1_patterns():
    """Enhanced schema creation using Epic 1 validated patterns"""
    
    # Step 1: Create base entity schemas with validated patterns
    create_crypto_entities_with_validation()
    
    # Step 2: Auto-generate kdata schemas using Epic 1 generator
    gen_crypto_kdata_schemas()
    
    # Step 3: Create tick-level schemas with data quality validation
    create_tick_schemas_with_quality_framework()
    
    # Step 4: Apply Epic 1 provider registration patterns
    register_crypto_providers_with_validation()
    
    print("‚úÖ Enhanced crypto schemas created with Epic 1 patterns")

def create_crypto_entities_with_validation():
    """Create entity schemas with Epic 1 validation patterns"""
    from sqlalchemy import create_engine, text
    
    # Validated DDL from Epic 1 architecture validation
    enhanced_crypto_asset_ddl = """
    CREATE TABLE crypto_asset (
        -- Epic 1 validated base fields
        id VARCHAR(128) PRIMARY KEY,
        entity_id VARCHAR(128) UNIQUE NOT NULL,
        entity_type VARCHAR(64) DEFAULT 'crypto',
        exchange VARCHAR(32) NOT NULL,
        code VARCHAR(64) NOT NULL,
        name VARCHAR(128),
        symbol VARCHAR(32) NOT NULL,
        
        -- Epic 1 enhanced metadata fields
        full_name VARCHAR(128),
        description TEXT,
        max_supply DECIMAL(36,8),
        circulating_supply DECIMAL(36,8),
        total_supply DECIMAL(36,8),
        market_cap DECIMAL(36,8),
        
        -- Epic 1 classification fields
        is_stablecoin BOOLEAN DEFAULT FALSE,
        consensus_mechanism VARCHAR(64),
        blockchain VARCHAR(64),
        contract_address VARCHAR(128),
        
        -- Epic 1 data quality fields
        data_quality_score FLOAT DEFAULT 1.0,
        last_validation_timestamp DATETIME,
        
        -- Standard ZVT fields
        list_date DATETIME,
        end_date DATETIME, 
        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
        
        -- Epic 1 validated indexes
        INDEX idx_crypto_asset_exchange_code (exchange, code),
        INDEX idx_crypto_asset_symbol (symbol),
        INDEX idx_crypto_asset_type (entity_type),
        INDEX idx_crypto_asset_timestamp (timestamp),
        INDEX idx_crypto_asset_active (end_date),
        
        -- Epic 1 data quality indexes  
        INDEX idx_crypto_asset_quality (data_quality_score),
        INDEX idx_crypto_asset_validation (last_validation_timestamp)
    );
    """
    
    # Execute with validation
    engine = get_db_engine(provider='zvt', data_schema=CryptoAsset)
    with engine.begin() as conn:
        conn.execute(text(enhanced_crypto_asset_ddl))
```

### Enhanced Data Quality Migration Patterns

#### Data Quality Schema Extensions
```sql
-- Epic 1 data quality tracking tables
CREATE TABLE crypto_data_quality_log (
    id VARCHAR(128) PRIMARY KEY,
    entity_id VARCHAR(128),
    provider VARCHAR(32),
    validation_type VARCHAR(64),
    validation_result JSON,
    quality_score FLOAT,
    issues_detected JSON,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_quality_log_entity (entity_id),
    INDEX idx_quality_log_provider (provider),
    INDEX idx_quality_log_score (quality_score),
    INDEX idx_quality_log_timestamp (timestamp)
);

-- Epic 1 cross-exchange consistency tracking  
CREATE TABLE crypto_price_consistency (
    id VARCHAR(128) PRIMARY KEY,
    symbol VARCHAR(64),
    timestamp_window DATETIME,
    provider_prices JSON,
    max_deviation_pct FLOAT,
    outlier_providers JSON,
    confidence_score FLOAT,
    recommended_price DECIMAL(36,8),
    validation_timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_consistency_symbol_time (symbol, timestamp_window),
    INDEX idx_consistency_deviation (max_deviation_pct),
    INDEX idx_consistency_confidence (confidence_score)
);
```

### Enhanced Performance Optimization

#### Epic 1 Validated Partitioning Strategy
```sql
-- Time-based partitioning for large kdata tables (Epic 1 optimization)
CREATE TABLE cryptopair_1m_kdata (
    -- Standard fields...
    id VARCHAR(128),
    entity_id VARCHAR(128), 
    timestamp DATETIME,
    -- OHLCV fields...
    
    PRIMARY KEY (id, timestamp)
) PARTITION BY RANGE (YEAR(timestamp)) (
    PARTITION p2024 VALUES LESS THAN (2025),
    PARTITION p2025 VALUES LESS THAN (2026),
    PARTITION p2026 VALUES LESS THAN (2027),
    PARTITION pmax VALUES LESS THAN MAXVALUE
);

-- Epic 1 validated composite indexes for query patterns
CREATE INDEX idx_crypto_kdata_entity_time_comp ON cryptopair_1m_kdata 
    (entity_id, timestamp, provider) 
    USING BTREE;

CREATE INDEX idx_crypto_kdata_symbol_range ON cryptopair_1m_kdata
    (code, timestamp, open, close)
    USING BTREE;
```

### Enhanced Security Migration

#### API Key Storage Enhancement
```sql
-- Epic 1 enhanced API key management
CREATE TABLE crypto_api_keys (
    id VARCHAR(128) PRIMARY KEY,
    provider VARCHAR(32) NOT NULL,
    environment VARCHAR(16) NOT NULL, -- dev, staging, prod
    key_type VARCHAR(16) NOT NULL, -- readonly, trading
    encrypted_key TEXT NOT NULL,
    encrypted_secret TEXT,
    encrypted_passphrase TEXT,
    encryption_version VARCHAR(16) DEFAULT 'aes256_v1',
    
    -- Epic 1 security enhancements
    ip_whitelist JSON,
    permissions JSON,
    rate_limits JSON,
    
    -- Epic 1 audit fields
    created_by VARCHAR(128),
    last_used_timestamp DATETIME,
    usage_count BIGINT DEFAULT 0,
    
    -- Epic 1 rotation management
    rotation_schedule VARCHAR(32), -- '90_days'
    next_rotation_date DATE,
    rotation_status VARCHAR(16) DEFAULT 'active',
    
    created_timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_timestamp DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    
    -- Epic 1 security indexes
    UNIQUE INDEX idx_api_keys_provider_env_type (provider, environment, key_type),
    INDEX idx_api_keys_rotation (next_rotation_date, rotation_status),
    INDEX idx_api_keys_usage (last_used_timestamp)
);
```

### Monitoring Integration Migration

#### Epic 1 Operational Tables
```sql
-- Epic 1 monitoring and metrics tables
CREATE TABLE crypto_operational_metrics (
    id VARCHAR(128) PRIMARY KEY,
    metric_name VARCHAR(64) NOT NULL,
    provider VARCHAR(32),
    metric_value FLOAT,
    metric_tags JSON,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_metrics_name_time (metric_name, timestamp),
    INDEX idx_metrics_provider (provider),
    INDEX idx_metrics_timestamp (timestamp)
);

-- Epic 1 WebSocket connection tracking
CREATE TABLE crypto_websocket_sessions (
    id VARCHAR(128) PRIMARY KEY,
    provider VARCHAR(32) NOT NULL,
    session_id VARCHAR(128),
    connection_status VARCHAR(16),
    subscribed_streams JSON,
    last_message_timestamp DATETIME,
    reconnect_count INTEGER DEFAULT 0,
    error_count INTEGER DEFAULT 0,
    
    started_timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    updated_timestamp DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
    
    INDEX idx_websocket_provider_status (provider, connection_status),
    INDEX idx_websocket_last_message (last_message_timestamp)
);
```

### Enhanced Validation and Testing

#### Migration Validation Framework
```python
# Epic 1 enhanced validation utilities
class Epic1MigrationValidator:
    """Enhanced migration validation using Epic 1 patterns"""
    
    def __init__(self):
        self.quality_validator = CryptoDataQualityValidator()
        self.architecture_validator = ArchitectureComplianceValidator()
    
    async def validate_migration_integrity(self) -> Dict:
        """Comprehensive migration validation"""
        
        validations = [
            self.validate_schema_compliance(),
            self.validate_data_quality_integration(),
            self.validate_provider_framework_setup(),
            self.validate_security_enhancements(),
            self.validate_monitoring_integration(),
            self.validate_performance_optimizations()
        ]
        
        results = await asyncio.gather(*validations)
        
        return {
            'overall_status': all(r['status'] == 'pass' for r in results),
            'detailed_results': dict(zip([
                'schema_compliance',
                'data_quality', 
                'provider_framework',
                'security',
                'monitoring',
                'performance'
            ], results))
        }
    
    async def validate_schema_compliance(self) -> Dict:
        """Validate Epic 1 schema compliance"""
        return await self.architecture_validator.validate_zv_patterns()
    
    async def validate_data_quality_integration(self) -> Dict:
        """Validate data quality framework integration"""
        # Test data quality validation endpoints
        # Test cross-exchange consistency tracking
        # Test quality score calculations
        pass
```

### Enhanced Rollback Procedures

#### Epic 1 Smart Rollback Strategy  
```python
def enhanced_rollback_with_data_preservation():
    """Smart rollback preserving valuable data"""
    
    # Epic 1 enhancement: Preserve valuable data during rollback
    backup_tables = [
        'crypto_data_quality_log',
        'crypto_price_consistency', 
        'crypto_operational_metrics'
    ]
    
    # Create backup before rollback
    for table in backup_tables:
        execute_sql(f"""
            CREATE TABLE {table}_backup_$(date +%s) AS 
            SELECT * FROM {table}
        """)
    
    # Standard rollback procedures
    standard_crypto_rollback()
    
    # Option to restore valuable operational data
    print("Backup tables created for potential data restoration")
```

### Production Deployment Enhancements

#### Epic 1 Blue-Green Deployment Pattern
```python
# Enhanced deployment with Epic 1 patterns
class Epic1CryptoDeployment:
    """Enhanced deployment using Epic 1 patterns"""
    
    def deploy_with_validation(self):
        steps = [
            self.validate_epic1_prerequisites(),
            self.deploy_schema_with_quality_framework(),
            self.initialize_provider_framework(),
            self.setup_monitoring_and_metrics(),
            self.validate_security_implementation(),
            self.run_comprehensive_health_checks(),
            self.perform_load_testing()
        ]
        
        for step in steps:
            result = step()
            if not result.success:
                self.rollback_deployment()
                raise DeploymentError(f"Failed at step: {step.__name__}")
                
        self.mark_deployment_successful()
```

## Epic 1 Success Metrics Integration

### Enhanced Acceptance Criteria
- ‚úÖ **Epic 1 Pattern Compliance**: 100% compliance with validated architecture patterns
- ‚úÖ **Data Quality Framework**: Operational data quality monitoring and validation
- ‚úÖ **Provider Framework Integration**: BaseCryptoProvider patterns implemented
- ‚úÖ **Security Enhancements**: API key encryption and audit logging operational
- ‚úÖ **Monitoring Integration**: CryptoMetrics Prometheus patterns active
- ‚úÖ **Performance Optimization**: Epic 1 validated query performance targets met

---

**Enhanced Implementation Status**: Migration strategy enhanced with Epic 1 validated patterns ready for Epic 2 deployment

**Next Steps:**
1. ‚úÖ Epic 1 Complete: All migration patterns validated
2. üöÄ Epic 2 Ready: Execute enhanced migration with validation framework
3. ‚è≥ Testing: Deploy to staging with Epic 1 enhancements
4. ‚è≥ Production: Blue-green deployment with comprehensive monitoring